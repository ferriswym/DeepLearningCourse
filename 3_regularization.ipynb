{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 697.482422\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 29.5%\n",
      "Minibatch loss at step 500: 201.003159\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 116.173790\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 68.660751\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 41.349365\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2500: 25.173788\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.488243\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weight1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_node]))\n",
    "  biase1 = tf.Variable(tf.zeros([hidden_node]))\n",
    "  weight2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_node, num_labels]))\n",
    "  biase2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  def model(data):\n",
    "    hidden = tf.nn.relu(tf.matmul(data, weight1) + biase1)\n",
    "    logits = tf.matmul(hidden, weight2) + biase2\n",
    "    return logits\n",
    "  logits = model(tf_train_dataset)\n",
    "  l2_loss = tf.nn.l2_loss(tf.concat([tf.reshape(weight1, [-1]), tf.reshape(weight2, [-1])], 0))\n",
    "  beta = 0.001\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta * l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 604.838989\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 500: 190.421448\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1000: 115.482323\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1500: 70.026505\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2000: 42.463715\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 2500: 25.784140\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 15.619398\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset[:2000, :]\n",
    "train_labels = train_labels[:2000, :]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 740.039917\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 500: 218.496384\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 117.875648\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1500: 69.857185\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2000: 41.389874\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2500: 25.240374\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 3000: 15.467460\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weight1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_node]))\n",
    "  biase1 = tf.Variable(tf.zeros([hidden_node]))\n",
    "  weight2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_node, num_labels]))\n",
    "  biase2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  def model(data):\n",
    "    hidden = tf.nn.relu(tf.matmul(data, weight1) + biase1)\n",
    "    logits = tf.nn.dropout(hidden, 0.5)\n",
    "    logits = tf.matmul(logits, weight2) + biase2\n",
    "    return logits\n",
    "  def model_test(data):\n",
    "    hidden = tf.nn.relu(tf.matmul(data, weight1) + biase1)\n",
    "    logits = tf.nn.dropout(hidden, 1)\n",
    "    logits = tf.matmul(logits, weight2) + biase2\n",
    "    return logits\n",
    "  logits = model(tf_train_dataset)\n",
    "  l2_loss = tf.nn.l2_loss(tf.concat([tf.reshape(weight1, [-1]), tf.reshape(weight2, [-1])], 0))\n",
    "  beta = 0.001\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta * l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model_test(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model_test(tf_test_dataset))\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.616313\n",
      "Minibatch accuracy: 13.0%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 1000: 0.696674\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2000: 0.778872\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 0.717183\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4000: 0.771116\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 5000: 0.566869\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6000: 0.600792\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 7000: 0.587059\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 8000: 0.534830\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.592006\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 10000: 0.527758\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11000: 0.516743\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 12000: 0.477456\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 13000: 0.468404\n",
      "Minibatch accuracy: 96.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 14000: 0.481608\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 15000: 0.482763\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 16000: 0.565189\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 17000: 0.476919\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 18000: 0.473560\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 19000: 0.417502\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 20000: 0.425404\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 90.7%\n",
      "Test accuracy: 96.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5dfed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "hidden_node = 1024\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weight = tf.Variable(\n",
    "    xavier_init(image_size * image_size, 4 * hidden_node))\n",
    "  input_bias = tf.Variable(tf.zeros([4 * hidden_node]))\n",
    "  hidden_weight1 = tf.Variable(\n",
    "    xavier_init(4 * hidden_node, 3 * hidden_node))\n",
    "  hidden_bias1 = tf.Variable(tf.zeros([3 * hidden_node]))\n",
    "  hidden_weight2 = tf.Variable(\n",
    "    xavier_init(3 * hidden_node, hidden_node))\n",
    "  hidden_bias2 = tf.Variable(tf.zeros([hidden_node]))\n",
    "  hidden_weight3 = tf.Variable(\n",
    "    xavier_init(hidden_node, num_labels))\n",
    "  hidden_bias3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  def model(data):\n",
    "    hidden1 = tf.nn.relu(tf.matmul(data, input_weight) + input_bias)\n",
    "    dropout1 = tf.nn.dropout(hidden1, 1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(dropout1, hidden_weight1) + hidden_bias1)\n",
    "    dropout2 = tf.nn.dropout(hidden2, 1)\n",
    "    hidden3 = tf.nn.relu(tf.matmul(dropout2, hidden_weight2) + hidden_bias2)\n",
    "    dropout3 = tf.nn.dropout(hidden3, 1)\n",
    "    logits = tf.matmul(dropout3, hidden_weight3) + hidden_bias3\n",
    "    return logits\n",
    "  def model_test(data):\n",
    "    hidden1 = tf.nn.relu(tf.matmul(data, input_weight) + input_bias)\n",
    "    dropout1 = tf.nn.dropout(hidden1, 1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(dropout1, hidden_weight1) + hidden_bias1)\n",
    "    dropout2 = tf.nn.dropout(hidden2, 1)\n",
    "    hidden3 = tf.nn.relu(tf.matmul(dropout2, hidden_weight2) + hidden_bias2)\n",
    "    dropout3 = tf.nn.dropout(hidden3, 1)\n",
    "    logits = tf.matmul(dropout3, hidden_weight3) + hidden_bias3\n",
    "    return logits\n",
    "  logits = model(tf_train_dataset)\n",
    "  l2_loss = tf.nn.l2_loss(tf.concat([tf.reshape(input_weight, [-1]), tf.reshape(hidden_weight1, [-1]), tf.reshape(hidden_weight2, [-1]), tf.reshape(hidden_weight3, [-1])], 0))\n",
    "  beta = 0.0001\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta * l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.03, global_step, 1000, 0.96)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#  optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model_test(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model_test(tf_test_dataset))\n",
    "    \n",
    "num_steps = 20001\n",
    "train_accuracy = [0.0]\n",
    "valid_accuracy = [0.0]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      accuracy1 = accuracy(predictions, batch_labels)\n",
    "      accuracy2 = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      train_accuracy.append(accuracy1)\n",
    "      valid_accuracy.append(accuracy2)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy1)\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy2)\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  train_plot = tf.constant(train_accuracy).eval()\n",
    "  valid_plot = tf.constant(valid_accuracy).eval()\n",
    "    \n",
    "  # plot\n",
    "  plotx = np.arange(0, num_steps, 1000)\n",
    "  plt.plot(plotx, train_plot[1:], color=\"#348ABD\", label=\"training\")\n",
    "  plt.plot(plotx, valid_plot[1:], color=\"#A60628\", label=\"validation\")\n",
    "  plt.legend()\n",
    "  plt.xlabel(\"Training iterations\")\n",
    "  plt.ylabel(\"Error rate\")\n",
    "  plt.title(\"Learning curve\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
